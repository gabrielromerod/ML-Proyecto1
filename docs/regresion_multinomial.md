# Clasificaci贸n de Mariposas utilizando Regresi贸n Log铆stica Multinomial  
El presente proyecto se centra en la emocionante tarea de clasificar diversas especies de mariposas, un desaf铆o que involucra la aplicaci贸n de t茅cnicas avanzadas de aprendizaje autom谩tico. En particular, abordamos esta tarea utilizando un enfoque de regresi贸n log铆stica multinomial.

La regresi贸n log铆stica es un algoritmo de aprendizaje supervisado ampliamente conocido, com煤nmente utilizado para resolver problemas de clasificaci贸n binaria. Sin embargo, en este contexto, nos enfrentamos a la tarea de clasificar m谩s de dos clases diferentes de mariposas, lo que requiere la extensi贸n de nuestro modelo para manejar "n" clases. Para abordar esta complejidad, hemos optado por la implementaci贸n de la regresi贸n log铆stica multinomial.

A lo largo de este informe, exploraremos en detalle c贸mo la regresi贸n log铆stica multinomial nos permite llevar a cabo esta tarea de clasificaci贸n de mariposas con 茅xito, considerando tanto su funcionamiento interno como su aplicaci贸n pr谩ctica en este emocionante proyecto de investigaci贸n.

## Funcionamiento de la Regresi贸n Log铆stica Multinomial:

### Hiperplano
En el contexto de la regresi贸n log铆stica multinomial y otros modelos de clasificaci贸n, el hiperplano es un concepto geom茅trico que se utiliza para separar las diferentes clases de datos en un espacio de caracter铆sticas. En este espacio, cada dimensi贸n corresponde a una caracter铆stica o atributo de los datos.

El hiperplano es una superficie que tiene una dimensi贸n menos que el espacio en el que se encuentra. Por ejemplo, en un espacio bidimensional (dos caracter铆sticas), un hiperplano ser铆a una l铆nea recta que separa dos clases. En un espacio tridimensional (tres caracter铆sticas), un hiperplano ser铆a un plano bidimensional que divide el espacio en dos regiones.

El objetivo de la regresi贸n log铆stica multinomial es encontrar el hiperplano 贸ptimo que mejor separe las clases de datos. Esto se logra ajustando los par谩metros del modelo (los pesos) de manera que el hiperplano tenga la mejor capacidad de separaci贸n posible.

- Implementaci贸n:
```python
def hiperplano(self):
    return np.dot(self.x, self.weights)
```
### Funci贸n Softmax
La funci贸n softmax se utiliza para convertir un conjunto de valores (a menudo llamados logits o puntuaciones) en una distribuci贸n de probabilidades sobre varias clases diferentes. Es especialmente 煤til en problemas de clasificaci贸n multiclase, donde se deben asignar probabilidades a m煤ltiples clases en lugar de solo dos, como en la clasificaci贸n binaria.

La funci贸n softmax se define matem谩ticamente como sigue:
Dada una matriz de logits Z de forma mn, donde m es el n煤mero de ejemplos y n es el n煤mero de clases, la funci贸n softmax calcula las probabilidades P de que cada ejemplo pertenezca a cada una de las n clases de la siguiente manera:
![Softmax](https://imgs.search.brave.com/lMajfuwzabVxlYRlYGQ0Pb9eJIfJm7YC1zpSGVXTCRg/rs:fit:500:0:0/g:ce/aHR0cHM6Ly9jZG4u/c2FuaXR5LmlvL2lt/YWdlcy92cjhncnU5/NC9wcm9kdWN0aW9u/LzU4MmE2YzUxNzAx/YmI1ODRjMWNkZDY2/NjJjYzM3NmI5Y2Fk/YjcxNjAtMjA0OHgx/MTUyLnBuZw)

La funci贸n softmax transforma los logits en una distribuci贸n de probabilidades de tal manera que las clases con logits m谩s altos tendr谩n mayores probabilidades asociadas. Esto significa que la clase con el logit m谩s alto ser谩 la clase m谩s probable seg煤n el modelo.

- Implementaci贸n:
```python
def softmax(self):
    logits = self.hiperplano()
    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
    return probabilities
```

### Funci贸n de p茅rdida
En la regresi贸n log铆stica multinomial, la funci贸n de p茅rdida se utiliza para evaluar cu谩n buenas son las predicciones del modelo en problemas de clasificaci贸n multiclase.
Durante el entrenamiento de la regresi贸n log铆stica multinomial, el objetivo es minimizar la funci贸n de p茅rdida. Esto se logra ajustando los pesos del modelo de manera que las predicciones se acerquen lo m谩s posible a los valores reales.
- Implementaci贸n:
```python
    def loss_function(self):
        probabilities = self.softmax()
        n = self.x.shape[0]
        loss = -np.sum(np.log(probabilities[range(n), self.y])) / n
        return loss
```

### Gradiente
La funci贸n de gradiente en la regresi贸n log铆stica multinomial se utiliza para calcular la direcci贸n y la magnitud del cambio necesario en los pesos del modelo para minimizar la funci贸n de p茅rdida y, por lo tanto, mejorar la calidad de las predicciones del modelo.
El gradiente se calcula para cada peso en el modelo en relaci贸n con la funci贸n de p茅rdida total. El gradiente es un vector que apunta en la direcci贸n de m谩ximo aumento de la funci贸n de p茅rdida, y su magnitud determina la tasa de cambio.

$\nabla L = \frac{1}{n} X^T \cdot (\hat{Y} - Y)$

Donde:
- $\nabla$ es el vector gradiente de la funci贸n de p茅rdida.

- n es el n煤mero de ejemplos de entrenamiento.

- X es la matriz de caracter铆sticas de entrenamiento.

- $\hat{Y}$ es la matriz de probabilidades predichas por el modelo (resultado de la funci贸n softmax).

- Y es la matriz de etiquetas reales.

- Implementaci贸n:
```python
def gradient(self):
    probabilities = self.softmax()
    n = self.x.shape[0]
    gradient = np.dot(self.x.T, (probabilities - np.eye(self.num_classes)[self.y])) / n
    return gradient
```
### Change parameters
Durante el proceso de entrenamiento, utilizamos la funci贸n change_parameters para actualizar los pesos del modelo. Esta funci贸n toma el gradiente calculado con respecto a la funci贸n de p茅rdida y los pesos actuales como entrada, y realiza una actualizaci贸n de los pesos utilizando una tasa de aprendizaje (伪). La tasa de aprendizaje controla la velocidad a la que los pesos del modelo convergen hacia los valores 贸ptimos. El objetivo es minimizar la funci贸n de p茅rdida y mejorar la precisi贸n de las predicciones a medida que avanzamos en el proceso de entrenamiento.
- Implementaci贸n:
```python
def change_parameters(self, gradient, w):
    return w - self.alpha * gradient
```
### Train
La funci贸n train es responsable de entrenar el modelo de regresi贸n log铆stica multinomial. Durante el entrenamiento, se ajustan los pesos del modelo de acuerdo con los datos de entrenamiento proporcionados. El proceso implica iterar a trav茅s de los datos varias veces (llamadas "茅pocas") y actualizar los pesos para minimizar la funci贸n de p茅rdida. El entrenamiento se lleva a cabo de la siguiente manera:

Inicializaci贸n: Al comienzo del entrenamiento, los pesos del modelo se inicializan t铆picamente en cero o con valores aleatorios peque帽os.

Iteraciones: Durante cada iteraci贸n, se toma una muestra aleatoria de los datos de entrenamiento (esto se conoce como "mini-lote" o "mini-batch"). Luego, se calcula el gradiente de la funci贸n de p茅rdida con respecto a los pesos actuales del modelo utilizando esta muestra.

Actualizaci贸n de pesos: Los pesos del modelo se actualizan utilizando el gradiente calculado y una tasa de aprendizaje (伪). Esta actualizaci贸n mueve los pesos en la direcci贸n que reduce la funci贸n de p茅rdida.

Repetici贸n: Este proceso se repite durante varias 茅pocas o hasta que se alcance un criterio de convergencia, como una p茅rdida m铆nima o un n煤mero m谩ximo de iteraciones.
- Implementaci贸n:
```python
def train(self):
    n = len(self.x)
    for _ in range(self.epochs):
        sample_idx = np.random.choice(n, n, replace=True)
        x_train, y_sample = self.x[sample_idx], self.y[sample_idx]
        gradient = self.gradient()
        self.weights = self.change_parameters(gradient, self.weights)
```
### Predict
La funci贸n predict toma las caracter铆sticas de prueba como entrada, calcula las probabilidades de pertenencia a cada clase utilizando el hiperplano ponderado por los pesos del modelo y selecciona la clase con la probabilidad m谩s alta como la predicci贸n final para cada instancia de prueba. Esto permite que el modelo realice predicciones sobre nuevos datos de entrada.
- Implementaci贸n:
```python
def predict(self, x_test):
    probabilities = np.dot(x_test, self.weights)
    predicted_classes = np.argmax(probabilities, axis=1)
    return predicted_classes
```